{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comparative-animal",
   "metadata": {},
   "source": [
    "# Even number GAN\n",
    "Below we collected into a single notebook this [python code](https://github.com/nbertagnolli/pytorch-simple-gan) of an author who wrote a blog post on this [simple GAN](https://towardsdatascience.com/build-a-super-simple-gan-in-pytorch-54ba349920e4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "departmental-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models import Discriminator, Generator\n",
    "from utils import generate_even_data, convert_float_matrix_to_int_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-grounds",
   "metadata": {},
   "source": [
    "Trains the even GAN. \n",
    "\n",
    "    Args:\n",
    "        max_int: The maximum integer our dataset goes to.  It is used to set the size of the binary\n",
    "            lists\n",
    "        batch_size: The number of examples in a training batch\n",
    "        training_steps: The number of steps to train on.\n",
    "        learning_rate: The learning rate for the generator and discriminator\n",
    "        print_output_every_n_steps: The number of training steps before we print generated output\n",
    "\n",
    "    Returns:\n",
    "        generator: The trained generator model\n",
    "        discriminator: The trained discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polish-dealer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 100, 98, 37, 115, 84, 66, 99, 103, 109, 115, 33, 100, 115, 114, 96]\n",
      "[99, 98, 115, 100, 69, 85, 119, 100, 33, 98, 103, 64, 96, 100, 102, 117]\n",
      "[103, 101, 103, 99, 68, 103, 100, 64, 84, 118, 100, 111, 69, 99, 98, 99]\n",
      "[103, 101, 97, 97, 103, 99, 99, 96, 101, 102, 100, 101, 98, 103, 117, 34]\n",
      "[103, 33, 103, 101, 64, 68, 100, 99, 100, 102, 99, 101, 103, 100, 101, 96]\n",
      "[101, 100, 101, 103, 98, 96, 99, 117, 101, 99, 97, 69, 96, 101, 101, 99]\n",
      "[99, 34, 101, 99, 101, 101, 101, 97, 100, 101, 101, 97, 113, 97, 101, 97]\n",
      "[100, 117, 117, 97, 100, 101, 101, 117, 99, 100, 99, 100, 100, 101, 101, 117]\n",
      "[101, 101, 99, 101, 103, 101, 99, 101, 97, 68, 101, 101, 100, 101, 100, 101]\n",
      "[101, 103, 101, 101, 33, 101, 101, 97, 101, 97, 101, 37, 117, 97, 100, 101]\n",
      "[101, 101, 113, 101, 101, 101, 101, 101, 100, 101, 99, 99, 101, 101, 101, 101]\n",
      "[99, 101, 103, 101, 97, 101, 101, 101, 101, 100, 101, 103, 101, 103, 100, 101]\n",
      "[97, 101, 101, 101, 117, 103, 101, 101, 101, 103, 101, 101, 101, 101, 99, 101]\n",
      "[101, 100, 100, 97, 101, 101, 117, 101, 101, 103, 101, 101, 103, 103, 103, 101]\n",
      "[103, 101, 103, 103, 101, 117, 101, 101, 101, 103, 103, 101, 103, 101, 99, 103]\n",
      "[103, 101, 103, 103, 99, 103, 103, 103, 103, 99, 103, 103, 103, 103, 33, 101]\n",
      "[101, 103, 99, 103, 103, 103, 101, 101, 103, 103, 101, 103, 103, 103, 103, 103]\n",
      "[103, 99, 99, 103, 99, 103, 103, 103, 103, 103, 103, 99, 101, 103, 115, 103]\n",
      "[103, 97, 103, 103, 103, 103, 103, 103, 103, 103, 115, 103, 103, 99, 103, 103]\n",
      "[103, 103, 103, 103, 103, 103, 103, 103, 119, 103, 103, 99, 103, 101, 99, 103]\n",
      "[103, 101, 103, 103, 103, 103, 119, 117, 117, 103, 103, 103, 103, 103, 103, 103]\n",
      "[103, 103, 103, 103, 101, 99, 103, 101, 99, 103, 103, 103, 103, 103, 103, 103]\n",
      "[103, 101, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103]\n",
      "[103, 103, 99, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 99, 103, 101]\n",
      "[103, 119, 103, 103, 103, 103, 119, 103, 103, 103, 99, 103, 103, 103, 119, 103]\n",
      "[103, 99, 103, 103, 103, 101, 119, 99, 103, 103, 99, 103, 117, 103, 99, 119]\n",
      "[99, 103, 103, 103, 103, 103, 103, 115, 103, 103, 103, 103, 103, 103, 119, 103]\n",
      "[103, 103, 103, 119, 103, 103, 119, 103, 103, 99, 103, 119, 99, 103, 103, 99]\n",
      "[103, 119, 119, 119, 115, 115, 103, 103, 119, 101, 103, 103, 119, 101, 103, 119]\n",
      "[119, 117, 101, 101, 103, 119, 103, 103, 99, 117, 117, 119, 103, 103, 103, 117]\n",
      "[103, 99, 103, 103, 119, 99, 119, 101, 117, 117, 103, 119, 103, 119, 103, 117]\n",
      "[117, 101, 119, 103, 119, 103, 117, 101, 103, 119, 119, 117, 117, 117, 101, 117]\n",
      "[117, 117, 117, 115, 103, 101, 99, 119, 117, 101, 101, 101, 119, 101, 103, 117]\n",
      "[117, 119, 119, 101, 101, 117, 119, 117, 101, 113, 117, 101, 117, 117, 101, 119]\n",
      "[117, 117, 119, 115, 101, 117, 117, 115, 117, 117, 117, 103, 117, 119, 117, 101]\n",
      "[117, 99, 117, 115, 117, 119, 117, 99, 117, 117, 119, 117, 117, 117, 117, 117]\n",
      "[117, 97, 117, 119, 117, 117, 117, 101, 117, 117, 101, 117, 117, 117, 117, 117]\n",
      "[117, 117, 113, 117, 101, 117, 117, 117, 117, 117, 117, 117, 113, 117, 117, 101]\n",
      "[113, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 37, 117, 113, 113, 117]\n",
      "[113, 117, 113, 113, 117, 49, 113, 113, 113, 117, 113, 113, 117, 117, 53, 117]\n",
      "[113, 113, 117, 113, 113, 97, 117, 117, 113, 117, 117, 117, 113, 117, 117, 49]\n",
      "[113, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 117, 97, 117]\n",
      "[113, 117, 113, 113, 113, 113, 113, 113, 117, 117, 113, 113, 117, 113, 113, 113]\n",
      "[113, 49, 117, 113, 117, 113, 117, 113, 113, 113, 113, 117, 117, 117, 113, 117]\n",
      "[117, 117, 117, 117, 117, 112, 117, 49, 117, 117, 117, 49, 117, 113, 113, 113]\n",
      "[113, 117, 113, 117, 113, 113, 113, 113, 113, 53, 113, 117, 113, 117, 113, 117]\n",
      "[113, 53, 113, 113, 49, 117, 113, 113, 117, 113, 113, 117, 117, 49, 117, 113]\n",
      "[113, 113, 113, 113, 113, 53, 49, 49, 117, 117, 113, 113, 113, 49, 52, 113]\n",
      "[113, 113, 113, 113, 117, 113, 113, 113, 113, 49, 49, 113, 117, 49, 113, 113]\n",
      "[117, 115, 115, 113, 49, 49, 49, 51, 113, 57, 113, 49, 112, 121, 113, 113]\n"
     ]
    }
   ],
   "source": [
    "max_int = 128\n",
    "batch_size = 16\n",
    "training_steps = 500\n",
    "learning_rate = 0.001\n",
    "print_output_every_n_steps = 10\n",
    "\n",
    "    \n",
    "input_length = int(math.log(max_int, 2))\n",
    "\n",
    "    # Models\n",
    "generator = Generator(input_length)\n",
    "discriminator = Discriminator(input_length)\n",
    "\n",
    "    # Optimizers\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # loss\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "for i in range(training_steps):\n",
    "        # zero the gradients on each iteration\n",
    "    generator_optimizer.zero_grad()\n",
    "\n",
    "    # Create noisy input for generator\n",
    "    # Need float type instead of int\n",
    "    noise = torch.randint(0, 2, size=(batch_size, input_length)).float()\n",
    "    \n",
    "    # Generate examples of even real data\n",
    "    true_labels, true_data = generate_even_data(max_int, batch_size=batch_size)\n",
    "    true_labels = torch.tensor(true_labels).float()\n",
    "    true_data = torch.tensor(true_data).float()\n",
    "    \n",
    "    # Train the generator\n",
    "    # We invert the labels here and don't train the discriminator because we want the generator\n",
    "    # to make things the discriminator classifies as true.\n",
    "    G_of_noise = generator(noise)\n",
    "    D_of_G_of_noise = discriminator(G_of_noise)\n",
    "    generator_loss = loss(D_of_G_of_noise, true_labels)\n",
    "    generator_loss.backward()\n",
    "    generator_optimizer.step()\n",
    "\n",
    "    # Train the discriminator on the true/generated data\n",
    "    discriminator_optimizer.zero_grad()\n",
    "    true_discriminator_out = discriminator(true_data)\n",
    "    true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "    # add .detach() here think about this\n",
    "    generator_discriminator_out = discriminator(G_of_noise.detach())\n",
    "    generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
    "    discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2\n",
    "    discriminator_loss.backward()\n",
    "    discriminator_optimizer.step()\n",
    "    if i % print_output_every_n_steps == 0:\n",
    "        print(convert_float_matrix_to_int_list(G_of_noise))\n",
    "        #print(discriminator_loss.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-oliver",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
